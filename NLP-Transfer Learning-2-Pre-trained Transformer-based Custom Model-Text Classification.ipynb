{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCS5-9gTgHw9"
      },
      "source": [
        "# Transfer Learning using a Pre-trained Transformer-based Custom Model for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdh0fWRgHxB"
      },
      "source": [
        "**Approach**\n",
        "\n",
        "In this notebook, we solve a natural language processing (NLP) task of text classification by using a Transformer architecture-based **custom model**. We create the custom model by adding a task-specific head to the body of a pre-trained Transformer model. Then, the model is fine-tuned for transfer learning. The pre-trained model is obtained from the Huggingface library.\n",
        "\n",
        "\n",
        "\n",
        "- Step 1: Load the raw dataset\n",
        "- Step 2: Tokenize the raw dataset\n",
        "- Step 3: Create a data collator\n",
        "- Step 4: Create train and test dataset loader objects\n",
        "- Step 5: Define a class for the task-specific custom model\n",
        "- Step 6: Create and compile the model\n",
        "- Step 7: Train the model\n",
        "- Step 8: Model evaluation\n",
        "\n",
        "\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "MRPC (Microsoft Research Paraphrase Corpus) is a text classification dataset. It consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).\n",
        "\n",
        "MRPC is one of the 10 datasets composing the GLUE benchmark,\n",
        "which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.\n",
        "\n",
        "**Acknowledgmentgement**\n",
        "\n",
        "This notebook is adapted from the following resources.\n",
        "- https://huggingface.co/learn/nlp-course/chapter3/1?fw=tf\n",
        "\n",
        "- Natural Language Processing with Transformers (Revised Edition) By Lewis Tunstall, Leandro von Werra, Thomas Wolf (Oâ€™Reilly, 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwD10n9ngHxC",
        "outputId": "35941e1e-642d-4b85-f836-12124355e02b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store the Fine-Tuned Model on the Hugging Face Hub**\n",
        "\n",
        "We can store the fine-tuned model Hugging Face Hub cloud repository. This will make it easier to reuse the fine-tuned model.\n",
        "\n",
        "We will use push_to_hub API for this purpose. However, to use this utility, we need to have a Hugging Face account (sign up with a Hugging Face account at: https://huggingface.co/welcome). Then, get an authentication token and input the token after running the following cell.\n"
      ],
      "metadata": {
        "id": "xyD6Mj4WoXw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "vNjwrbwzouIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import TFAutoModel, AutoConfig\n",
        "import evaluate\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "KSfX8CohoUsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load the raw dataset**\n",
        "\n",
        "The load_dataset method returns a dictionary object of type DatasetDict."
      ],
      "metadata": {
        "id": "5FR_--Mxox2f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy8W6b4ZgHxD",
        "outputId": "5d095251-0764-4bd6-8115-d5774a840bff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 3668\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 408\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
            "        num_rows: 1725\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "print(raw_datasets)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Access each pair of sentences in the raw_datasets object by indexing\n",
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ],
      "metadata": {
        "id": "cYFGuu0Yg9WB",
        "outputId": "0f99256d-0394-4937-fae3-503163a4d206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the features of the raw_train_dataset object\n",
        "raw_train_dataset.features"
      ],
      "metadata": {
        "id": "OKLIGrVZhEzm",
        "outputId": "acde449b-f743-4d79-9005-6760fcec9fc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': Value(dtype='string', id=None),\n",
              " 'sentence2': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
              " 'idx': Value(dtype='int32', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Tokenize the raw dataset**\n",
        "\n",
        "Tokenization is the process of converting the text to numbers\n",
        "This will be done via a tokenizer object.\n",
        "\n",
        "- Step 2(a): Instantiate a tokenizer object\n",
        "- Step 2(b): Define a function to tokenize the input\n",
        "- Step 2(c): Tokenize the batches\n"
      ],
      "metadata": {
        "id": "nygLE3Puo2kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Step 2(a): Instantiate a tokenizer object\n",
        "This is done by using a suitable model checkpoint\n",
        "'''\n",
        "MODEEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEEL_CHECKPOINT)\n",
        "\n",
        "tokenizer.model_max_len = 512\n",
        "\n",
        "\n",
        "'''\n",
        "Step 2(b): Define a function to tokenize the input\n",
        "\n",
        "The truncation=True will truncate the sequences\n",
        "    that are longer than the model max length\n",
        "    (e.g., 512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "For truncating the sequences that are longer than the specified max length,\n",
        "use the following code:\n",
        "tokenizer(example[\"sentence1\"], example[\"sentence2\"],\n",
        "          max_length=8, truncation=True)\n",
        "'''\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"],\n",
        "                     truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "'''\n",
        "- Step 2(c): Tokenize the batches\n",
        "\n",
        "We want to keep the data as a dataset object after tokenization.\n",
        "So, we use the Dataset map() method for tokenization,\n",
        "by enabling it to utilize the tokenizer function.\n",
        "The map() method works by applying a function to each element of the dataset.\n",
        "This gives us the flexibility to apply additional preprocessing\n",
        "via the map() method.\n",
        "\n",
        "We set batched=True load samples in the RAM in batches.\n",
        "This is possible because the datasets from the ðŸ¤— Datasets library\n",
        "are stored on the disk as they are Apache Arrow files.\n",
        "\n",
        "Note that we didn't pad the samples.\n",
        "Because it's more efficient to apply padding during the creation of the batches.\n",
        "In such a case, we only need to pad to the maximum length in that batch,\n",
        "and not the maximum length in the entire dataset.\n",
        "This will save time and processing power\n",
        "when the inputs have very variable lengths!\n",
        "'''\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "9llfXLdWgzLY",
        "outputId": "b57cd7d3-7084-49ca-b63e-cbd9b864a901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "1d4a431ec12e440d90b061a441395f59",
            "0ff11e76d2a04b1fa10a00d52e8b5c69",
            "3640aa39b56a4b4ea1b55a9611bca768",
            "df841a1b19ae44488ddcccba6526f857",
            "55e697ef1772401c806937863d165527",
            "974873811b1f4f4eba65b15d333239b4",
            "d3bc10a0d56c4d79a1f314e3aaaa2ddc",
            "99ad9b58ee4949d2bc7a00cb4f1aff20",
            "ede583bc9b4243d4a205a05fdc28b5fc",
            "4dbf2a4af35a4087af53ca82715c1e96",
            "244d9f28a72a474cba90cafa18e04b72"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d4a431ec12e440d90b061a441395f59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create a data collator**\n",
        "\n",
        "Put together the samples in a batch by using a collate function.\n",
        "By default, this function converts samples to tf.Tensor and\n",
        "concatenate them (recursively if the elements are lists,\n",
        "tuples, or dictionaries).\n",
        "\n",
        "We can't utilize the default function as our inputs have variable lengths.\n",
        "To address the variable-length issue, we will apply padding during the batching.\n",
        "This is an efficient approach (padding during baching) as we can\n",
        "avoid having over-long inputs with a lot of padding.\n",
        "\n",
        "For applying the correct amount of padding to the items\n",
        "of the dataset in a batch we use DataCollatorWithPadding.\n",
        "The DataCollatorWithPadding takes a tokenizer when we instantiate it (to know which padding token to use,\n",
        "and whether the model expects padding to be on the left or on the right of the inputs) and will do everything we need.\n"
      ],
      "metadata": {
        "id": "XvkaTZsrpS8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "\n",
        "'''\n",
        "Inspect the collated batched data\n",
        "\n",
        "For inspecting the padding added via the collator function,\n",
        "we look at a few samples from our training set that we would like to batch together.\n",
        "Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed\n",
        "and contain strings (and we canâ€™t create tensors with strings) and\n",
        "have a look at the lengths of each entry in the batch.\n",
        "'''\n",
        "\n",
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "print([len(x) for x in samples[\"input_ids\"]])\n",
        "\n",
        "\n",
        "'''\n",
        "Check the dynamic padding of the batch via the data_collator.\n",
        "We will see that the length of each entry in the batch is set\n",
        "to the length of the max length entry in the same batch\n",
        "'''\n",
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}\n"
      ],
      "metadata": {
        "id": "_heEEwNdhrUp",
        "outputId": "5052bfa7-0db2-4ebd-ac06-4c0207397751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 59, 47, 67, 59, 50, 62, 32]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': TensorShape([8, 67]),\n",
              " 'attention_mask': TensorShape([8, 67]),\n",
              " 'labels': TensorShape([8])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**"
      ],
      "metadata": {
        "id": "H0xm5oTwplPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "MAX_EPOCHS = 4\n",
        "\n",
        "\n",
        "'''\n",
        "The initial learning rate is used by the optimizers, e.g., SGD, ADAM, NADAM, etc.\n",
        "\n",
        "Note that transformer models benefit from a much lower learning rate than the default for Adam, which is 1e-3,\n",
        "A much smaller rate, e.g., 5e-5, is a better starting point.\n",
        "'''\n",
        "INITIAL_LEARNING_RATE = 2e-5\n",
        "\n",
        "WEIGHT_DECAY = 0.01"
      ],
      "metadata": {
        "id": "FcSuCISRpj9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Create train and test dataset loader objects**\n",
        "\n",
        "Create the train and validation dataset by putting togerher the\n",
        "tokenized dataset (step 3) and collated dataset (step 4) via the to_tf_dataset() method.\n",
        "\n",
        "It will wrap a tf.data.Dataset around the dataset, with an optional collation function.\n",
        "\n",
        "The tf.data.Dataset is a native TensorFlow format that Keras can use for model.fit().\n"
      ],
      "metadata": {
        "id": "yMc7XY9SpaOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "LPhuDljArIsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a06fa6-e26b-4209-ea8a-15e638636f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
            "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
            "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
            "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Define a class for the task-specific custom model**\n",
        "\n",
        "\n",
        "The architecture of a Transformer model has two separable key components: body and head.\n",
        "- The last layer is called the model head and is task-specific\n",
        "- The rest of the model is called the body and it includes task-agnostic token embeddings and transformer layers\n",
        "\n",
        "We will create a custom model by adding a task-specific head to the body of a Transformer model.\n"
      ],
      "metadata": {
        "id": "obNxOTcLiPyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "  def __init__(self, checkpoint, num_labels, dropout_rate=0.1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_labels = num_labels\n",
        "\n",
        "    # Load Model with given checkpoint and extract its body\n",
        "    self.model = model = TFAutoModel.from_pretrained(checkpoint,\n",
        "                                                   config=AutoConfig.from_pretrained(checkpoint,\n",
        "                                                                                     output_attentions=True,\n",
        "                                                                                     output_hidden_states=True))\n",
        "    # Set up the model head (i.e., the classification layer)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.classifier = tf.keras.layers.Dense(num_labels) # load and initialize weights\n",
        "\n",
        "\n",
        "  def call(self, input_ids=None, attention_mask=None):\n",
        "\n",
        "    # Extract outputs from the body\n",
        "    # Use the model body to get encoder representations\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # Shape of output: batch_size, n_tokens, hidden_dim\n",
        "\n",
        "    # Pass the encoder representation of the last hidden state through the model head\n",
        "    sequence_output = self.dropout(outputs[0]) # outputs[0]=last hidden state\n",
        "\n",
        "    logits = self.classifier(sequence_output[:,0,:])\n",
        "    '''\n",
        "    The above expression sequence_output[:, 0, :] extracts the first token or element from each sequence in the batch.\n",
        "    In the BERT model, the first token is the representation of the [CLS] token in the input sequence.\n",
        "    The [CLS] token is commonly used for tasks like text classification.\n",
        "    '''\n",
        "\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "iuGWu59W1PE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Create and compile the model**\n",
        "\n",
        "Before we instantiate and compile the model, we need to define the optimizer\n",
        "and the loss function."
      ],
      "metadata": {
        "id": "TtMoUCX7p1-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sQJ4dl0gHxG",
        "outputId": "fe905181-aa44-47c9-cc21-1af86f434d75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "########################## Create the Model ##########################\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n########################## Create the Model ##########################\\n\\n\")\n",
        "\n",
        "'''\n",
        "Reset all states generated by Keras.\n",
        "It deletes the TensorFlow graph before creating a new model,\n",
        "otherwise memory overflow will occur.\n",
        "'''\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "'''\n",
        "To reproduce the same result by the model in each iteration, we use fixed seeds for random number generation.\n",
        "'''\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "###################### Optimizer ##########################\n",
        "# We provide various choices for the optimizer\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "For the learning schedule, we need to set how long training is going to be, i.e., the number of training steps.\n",
        "num_of_training_steps = (num_of_training_samples // batch_size) *  epochs\n",
        "\n",
        "Since the tf_train_dataset is batched, its len() is already num_of_training_samples // batch_size\n",
        "'''\n",
        "num_of_training_steps = len(tf_train_dataset) * MAX_EPOCHS\n",
        "\n",
        "'''\n",
        "Scheduler: ExponentialDecay\n",
        "'''\n",
        "lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=INITIAL_LEARNING_RATE,\n",
        "    decay_steps=num_of_training_steps,\n",
        "    decay_rate=WEIGHT_DECAY,\n",
        "    staircase=True)\n",
        "\n",
        "'''\n",
        "Scheduler: PolynomialDecay\n",
        "'''\n",
        "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=INITIAL_LEARNING_RATE,\n",
        "    decay_steps=num_of_training_steps,\n",
        "    end_learning_rate=0.0,\n",
        "    power=1.0,\n",
        "    cycle=False,\n",
        "    name=None\n",
        ")\n",
        "\n",
        "'''\n",
        "Optimizer:\n",
        "\n",
        "Instantiate an optimizer. Use one of the following choices.\n",
        "- Fixed LR: learning_rate=INITIAL_LEARNING_RATE\n",
        "- Scheduled LR: learning_rate=lr_scheduler\n",
        "'''\n",
        "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler, momentum=0.9, nesterov=False)\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
        "#optimizer=tf.keras.optimizers.Nadam(learning_rate=lr_scheduler)\n",
        "#optimizer=tfa.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=WEIGHT_DECAY)\n",
        "#optimizer=tfa.optimizers.LAMB(learning_rate=lr_scheduler, weight_decay_rate=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "###################### Loss Function ##########################\n",
        "\n",
        "'''\n",
        "Loss Function:\n",
        "\n",
        "Instantiate a function to compute the training loss (per iteration/step).\n",
        "NOTE: the \"reduction\" argument should be set to the value AUTO (it's the default value).\n",
        "AUTO indicates that the reduction option will be determined by the usage context.\n",
        "For almost all cases this defaults to SUM_OVER_BATCH_SIZE.\n",
        "Thus, the function will return a single scalar loss value for the entire batch.\n",
        "If \"reduction\" is set to NONE,\n",
        "then we need to apply tf.reduce_mean() function over all loss values for every instance in the batch.\n",
        "'''\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=tf.keras.losses.Reduction.AUTO)\n",
        "\n",
        "\n",
        "###################### Model Instatiation ##########################\n",
        "\n",
        "NUM_LABELS = 2\n",
        "'''\n",
        "Instantiate the custom model and compile it\n",
        "'''\n",
        "model=CustomModel(checkpoint=MODEEL_CHECKPOINT, num_labels=NUM_LABELS)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Callback Functions**\n",
        "\n",
        "Define the following callback function.\n",
        "- PushToHubCallback"
      ],
      "metadata": {
        "id": "39em3HKaqypJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PushToHubCallback\n",
        "It will sync up the fine-tuned model with the Hugging Face Hub.\n",
        "First, the model will be stored (serialized) on the disk (output_dir).\n",
        "Then, it will be synced.\n",
        "\n",
        "This function will allow model reuse.\n",
        "- The locally stored model can be loaded from \"output_dir\"\n",
        "- The cloud-stored model can be loaded from the Hub\n",
        "\n",
        "The function will allow to resume training from other machines,\n",
        "share the model after training is finished,\n",
        "and even test the model's inference quality midway through training!\n",
        "'''\n",
        "\n",
        "# Define a name of the fine-tuned model for the callback function\n",
        "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
        "push_to_hub_model_id = f\"{model_name}-finetuned-custom_model-classification_mrpc\"\n",
        "\n",
        "\n",
        "push_to_hub_callback = PushToHubCallback(\n",
        "    output_dir=\"./model_custom_classification_mrpc_save\",\n",
        "    tokenizer=tokenizer,\n",
        "    hub_model_id=push_to_hub_model_id,\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = [push_to_hub_callback]"
      ],
      "metadata": {
        "id": "hzGZt_yOqzrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Train the model**\n"
      ],
      "metadata": {
        "id": "0aZr5UYWq_OX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCfQ5uhjgHxI",
        "outputId": "64badf0b-74bc-4347-d423-10e0e33153d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "459/459 [==============================] - 93s 146ms/step - loss: 0.5417 - accuracy: 0.7249 - val_loss: 0.4212 - val_accuracy: 0.7868\n",
            "Epoch 2/3\n",
            "459/459 [==============================] - 45s 98ms/step - loss: 0.2876 - accuracy: 0.8732 - val_loss: 0.3350 - val_accuracy: 0.8431\n",
            "Epoch 3/3\n",
            "459/459 [==============================] - 46s 99ms/step - loss: 0.0853 - accuracy: 0.9719 - val_loss: 0.5166 - val_accuracy: 0.8382\n",
            "\n",
            "Saving the fully trained model in the SavedModel format ... \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x785d69193700>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x785cf942a560>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x785cf9439120>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x785cf943bca0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x785cf944e860>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x785cf945d420>, because it is not built.\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Train in mixed-precision float16\n",
        "Mixed precision is the use of both 16-bit and 32-bit floating-point types\n",
        "in a model during training to make it run faster and use less memory.\n",
        "'''\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(tf_train_dataset,\n",
        "          validation_data=tf_validation_dataset,\n",
        "          epochs=MAX_EPOCHS,\n",
        "          callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Model evaluation**\n",
        "\n",
        "We will use the predict() method of the fine-tuned model to return the logits from the output head of the model, one per class. Then, the logits will be converted into class probabilities, which will be used to compute the classification performance, i.e., accuracy and class-based precision, recall, and F1 score.\n",
        "\n",
        "We will use the following three approaches to obtain the fine-tuned model.\n",
        "- Use the current fine-tuned model\n",
        "- Load the saved fine-tuned model from the disk\n",
        "- Load the saved fine-tuned model from the Hugging Face Hub\n",
        "\n",
        "\n",
        "In addition, we can load the metrics associated with the MRPC dataset using the evaluate.load() function. The object returned has a compute() method we can use to do the metric calculation.\n",
        "\n",
        "\n",
        "**Step 7 (a): Use the current fine-tuned model**"
      ],
      "metadata": {
        "id": "RPNoqjyDrRda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the logits for the validation input\n",
        "logits_pred = model.predict(tf_validation_dataset)\n",
        "\n",
        "# Compute the predicted class labels\n",
        "labels_pred = np.argmax(logits_pred, axis=1)\n",
        "\n",
        "# Shape of the predicted logits\n",
        "print(\"\\nShape of the predicted logits: \", logits_pred.shape)\n",
        "\n",
        "# Shape of the predicted class labels\n",
        "print(\"Shape of the predicted class labels: \", labels_pred.shape)\n",
        "\n",
        "\n",
        "'''\n",
        "Create an array (NumPy) of validation labels from the validation data loader object\n",
        "'''\n",
        "# A list for storing the validation labels\n",
        "labels_list = []\n",
        "\n",
        "# Get the validation labels from the validation data loader object\n",
        "for batch in tf_validation_dataset:\n",
        "    labels_list_batch = batch[1]  # Extracting labels from the batch\n",
        "    labels_list.extend(labels_list_batch.numpy())  # Assuming labels are in a NumPy array, convert to Python list and extend the list\n",
        "\n",
        "# Number of validation labels in the list\n",
        "print(\"Number of validation labels in the list: \", len(labels_list))\n",
        "\n",
        "# Convert the val label list into a NumPy array\n",
        "labels = np.array(labels_list)\n",
        "\n",
        "\n",
        "# Shape of the labels array\n",
        "print(\"Shape of the labels array: \", labels.shape)\n",
        "\n",
        "\n",
        "# Classification report\n",
        "class_names = ['class 0', 'class 1']\n",
        "print(\"\\n-----------------------------------------------------------\\n\")\n",
        "print(classification_report(labels, labels_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rga0sMpXjkn",
        "outputId": "e0d6ca9a-28d2-453c-95cf-60b1dd917ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 2s 39ms/step\n",
            "Shape of the predicted logits:  (408, 2)\n",
            "Shape of the predicted class labels:  (408,)\n",
            "Number of validation labels in the list:  408\n",
            "Shape of the labels array:  (408,)\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.80      0.60      0.68       129\n",
            "     class 1       0.83      0.93      0.88       279\n",
            "\n",
            "    accuracy                           0.83       408\n",
            "   macro avg       0.82      0.76      0.78       408\n",
            "weighted avg       0.82      0.83      0.82       408\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 (b): Load the saved fine-tuned model from the disk**"
      ],
      "metadata": {
        "id": "QJVCR133ra5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the saved model on the disk\n",
        "output_dir=\"./model_saved_classification_mrpc_save\"\n",
        "NUM_LABELS = 2\n",
        "model_saved = TFAutoModelForSequenceClassification.from_pretrained(output_dir, num_labels=NUM_LABELS)\n",
        "logits_pred = model_saved.predict(tf_validation_dataset)[\"logits\"]"
      ],
      "metadata": {
        "id": "_adXIbH1reh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 (c): Load the saved fine-tuned model from the Hugging Face Hub**"
      ],
      "metadata": {
        "id": "YyIyBAiCrkqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned model stored on the Hugging Face Hub\n",
        "FINE_TUNED_MODEL = \"hasan-mr/distilbert-base-uncased-finetuned-custom_model-classification_mrpc\"\n",
        "NUM_LABELS = 2\n",
        "model_saved = TFAutoModelForSequenceClassification.from_pretrained(FINE_TUNED_MODEL, num_labels=NUM_LABELS)\n",
        "logits_pred = model_saved.predict(tf_validation_dataset)[\"logits\"]"
      ],
      "metadata": {
        "id": "qbyktnJrr1bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the metrics associated with the MRPC dataset**\n",
        "\n",
        "We use the evaluate.load() function to load the metrics associated with the MRPC dataset. The object returned has a compute() method that can be used to do the metric calculation."
      ],
      "metadata": {
        "id": "LIRCoDnwsCX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6GWaj1mgHxK",
        "outputId": "30f26cf3-4aec-44db-8d2b-e50c05e65eaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8259803921568627, 'f1': 0.8798646362098139}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=labels_pred, references=raw_datasets[\"validation\"][\"label\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d4a431ec12e440d90b061a441395f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ff11e76d2a04b1fa10a00d52e8b5c69",
              "IPY_MODEL_3640aa39b56a4b4ea1b55a9611bca768",
              "IPY_MODEL_df841a1b19ae44488ddcccba6526f857"
            ],
            "layout": "IPY_MODEL_55e697ef1772401c806937863d165527"
          }
        },
        "0ff11e76d2a04b1fa10a00d52e8b5c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_974873811b1f4f4eba65b15d333239b4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d3bc10a0d56c4d79a1f314e3aaaa2ddc",
            "value": "Map: 100%"
          }
        },
        "3640aa39b56a4b4ea1b55a9611bca768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ad9b58ee4949d2bc7a00cb4f1aff20",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ede583bc9b4243d4a205a05fdc28b5fc",
            "value": 408
          }
        },
        "df841a1b19ae44488ddcccba6526f857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dbf2a4af35a4087af53ca82715c1e96",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_244d9f28a72a474cba90cafa18e04b72",
            "value": " 408/408 [00:00&lt;00:00, 2362.31 examples/s]"
          }
        },
        "55e697ef1772401c806937863d165527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "974873811b1f4f4eba65b15d333239b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3bc10a0d56c4d79a1f314e3aaaa2ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99ad9b58ee4949d2bc7a00cb4f1aff20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede583bc9b4243d4a205a05fdc28b5fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dbf2a4af35a4087af53ca82715c1e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244d9f28a72a474cba90cafa18e04b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}