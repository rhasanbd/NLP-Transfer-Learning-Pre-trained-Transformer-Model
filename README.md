# Transfer Learning using a Pre-trained Transformer Model for Natural Language Processing

The notebooks in this series use pre-trained Transformer models from the Huggingface library to solve various NLP tasks.


- Notebook 1: NLP-Transfer Learning-1-Pre-trained Transformer Model-Text Classification

    -- Text classification by using an **encoder** Transformer architecture-based pre-trained model BERT. Specifically, the model is fine-tuned for transfer learning. 
  
- Notebook 2: NLP-Transfer Learning-2-Pre-trained Transformer-based Custom Model-Text Classification

    -- Text classification by using an **encoder** Transformer BERT-based **custom model**. We create the custom model by adding a task-specific head to the body of a pre-trained Transformer model. Then, the model is fine-tuned for transfer learning. 

  
- Notebook 3: NLP-Transfer Learning-3-Pre-trained Transformer Model-Text Summarization

    -- Text summarization by using an **encoder-decoder** Transformer architecture-based pre-trained model T5. Specifically, the T5 model is fine-tuned for transfer learning.
